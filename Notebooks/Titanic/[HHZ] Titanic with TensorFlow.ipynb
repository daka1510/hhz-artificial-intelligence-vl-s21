{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to classify structured data using Keras preprocessing layers. You will use [Keras](https://www.tensorflow.org/guide/keras) to define the model, and [preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) as a bridge to map from columns in a CSV to features used to train the model. \n",
    "\n",
    "The content is based on a [tutorial](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) from the TensorFlow team. Some utility functions used in this notebook are copied from this tutorial and we recommend to refer to it for more details.\n",
    "\n",
    "You will train and deploy a Neural Network to predict which passengers survived the Titanic shipwreck (see [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview) at Kaggle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Required TensorFlow version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code snippets used in this notebook only work with TensorFlow 2.3.1 and above.\n",
    "\n",
    "Uncomment and run the below upgrade command if needed and restart the kernel afterwards via `Kernel > Restart` to use the updated packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pandas to create a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import training data from public Github URL and load it into a dataframe.\n",
    "\n",
    "*In previous notebooks you imported the csv file from the project assets in Watson Studio. Below code snippet illustrates another way for the same task.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/daka1510/hhz-artificial-intelligence-vl-s21/main/Notebooks/Titanic/train.csv\"\n",
    "csv_file = tf.keras.utils.get_file(\"train.csv\", dataset_url, cache_dir=\".\")\n",
    "dataframe = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation for this dataset was covered in depth in a different notebook (see [HHZ - Titanic Data Preparation](https://github.com/daka1510/ai-workshop-hhz/blob/master/Notebooks/Titanic/%5BHHZ%5D%20Titanic%20Data%20Preparation.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column 'Cabin' since there are too many missing values\n",
    "dataframe = dataframe.drop([\"Cabin\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values for 'Embarked': use most frequent value\n",
    "dataframe[\"Embarked\"] = dataframe[\"Embarked\"].fillna(dataframe[\"Embarked\"].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values for 'Age': use mean value\n",
    "dataframe[\"Age\"] = dataframe[\"Age\"].fillna((dataframe[\"Age\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify results\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataframe into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "print(f\"{len(train)} train examples\")\n",
    "print(f\"{len(val)} validation examples\")\n",
    "print(f\"{len(test)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an input pipeline using tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will wrap the dataframes with tf.data, in order to shuffle and batch the data. If you were working with a very large CSV file (so large that it does not fit into memory), you would use tf.data to read it from disk directly. That is not covered in this tutorial.\n",
    "\n",
    "Note: Below utility functions are copied from https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers and have been adapted for our use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"Survived\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the input pipeline, let's call it to see the format of the data it returns. You have used a small batch size to keep the output readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print(f\"Every feature: {list(train_features.keys())}\")\n",
    "print(f'A batch of ages: {train_features[\"Age\"]}')\n",
    "print(f\"A batch of targets {label_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check documentation for details:\n",
    "# - https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization\n",
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for our feature.\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check documentation for details:\n",
    "# - https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding\n",
    "# - https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/IntegerLookup\n",
    "# - https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/StringLookup\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    if dtype == \"string\":\n",
    "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Create a Discretization for our integer indices.\n",
    "    encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices.\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "    # layer so we can use them, or include them in the functional model later.\n",
    "    return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a few examples to get an understanding of how the encoding works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a normalization layer for 'Age'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_col = train_features[\"Age\"]\n",
    "age_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = get_normalization_layer(\"Age\", train_ds)\n",
    "layer(age_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a category encoding layer for 'Embarked' (string)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_col = train_features[\"Embarked\"]\n",
    "embarked_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_encoding_layer = get_category_encoding_layer(\"Embarked\", train_ds, \"string\")\n",
    "category_encoding_layer(embarked_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a category encoding layer for 'Pclass' (numeric)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_col = train_features[\"Pclass\"]\n",
    "pclass_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_encoding_layer = get_category_encoding_layer(\"Pclass\", train_ds, \"int64\")\n",
    "category_encoding_layer(pclass_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose which columns to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen how to use several types of preprocessing layers. Now you will use them to train a model. You will be using [Keras-functional API](https://www.tensorflow.org/guide/keras/functional) to build the model. The Keras functional API is a way to create models that are more flexible than the [tf.keras.Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numeric features.\n",
    "for header in [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers.\n",
    "pclass_col = tf.keras.Input(shape=(1,), name=\"Pclass\", dtype=\"int64\")\n",
    "encoding_layer = get_category_encoding_layer(\"Pclass\", train_ds, dtype=\"int64\")\n",
    "encoded_pclass_col = encoding_layer(pclass_col)\n",
    "\n",
    "all_inputs.append(pclass_col)\n",
    "encoded_features.append(encoded_pclass_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features encoded as string.\n",
    "categorical_cols = [\"Sex\", \"Embarked\"]\n",
    "for header in categorical_cols:\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype=\"string\")\n",
    "    encoding_layer = get_category_encoding_layer(header, train_ds, dtype=\"string\")\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_features.append(encoded_categorical_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create, compile, and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x1 = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x1)\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review how training and validation accuracy evolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, \"darkgreen\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"darkblue\", label=\"Validation accuracy\")\n",
    "plt.plot(epochs, loss, \"lightgreen\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"lightblue\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different neural network configurations (e.g. add additional layers, change the number of neurons per layer, or train for more epochs). Do you get a better accuracy than using the initial configuration?\n",
    "\n",
    "Continue once you are satisfied with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make local prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to filesystem and reload it for test purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf titanic_model\n",
    "model.save('titanic_model/1', save_format='tf')\n",
    "reloaded_model = tf.keras.models.load_model('titanic_model/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a local prediction with the aid of the reloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_passenger_df = dataframe.iloc[1:2]\n",
    "sample_passenger_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_passenger_df = sample_passenger_df.drop([\"Survived\", \"Name\", \"Ticket\", \"PassengerId\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop target ('Survived') from passenger record\n",
    "sample_passenger_dict = sample_passenger_df.to_dict(orient=\"records\")[0]\n",
    "sample_passenger_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample_passenger_dict.items()}\n",
    "predictions = reloaded_model.predict(input_dict)\n",
    "\n",
    "print(\"This passenger had a %.1f percent probability of surviving the Titanic shipwreck.\" % (100 * predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing TensorFlow 2.3 was not yet supported by Watson Machine Learning (https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/pm_service_supported_frameworks.html?audience=wdp). In the following you will deploy your model with the aid of https://www.tensorflow.org/tfx/serving/docker.\n",
    "\n",
    "Working through the below section is **optional** and requires a local Docker installation (installation instructions available at the referenced [link](https://www.tensorflow.org/tfx/serving/docker#install_docker))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archive the model directory and export it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, download and unzip the exported model to your local machine. In Watson Studio, below code snippets help to export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review content of the model directory\n",
    "!ls -ll titanic_model/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip model directory\n",
    "!zip -r titanic_model_v1.zip titanic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review directory content\n",
    "!ls -ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this notebook in Watson Studio, make sure to run \"Insert project token\" first\n",
    "f = open(\"titanic_model_v1.zip\", \"rb\")\n",
    "project.save_data(\"titanic_model_v1.zip\", f.read(), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now navigate to your project's asset list, download and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Docker is installed, you can run below commands to start a serving image (update paths accordingly):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker pull tensorflow/serving\n",
    "docker run -t --rm -p 8501:8501 \\\n",
    "    -v \"/Users/dkaulen/Desktop/TF/titanic_model:/models/titanic_model\" \\\n",
    "    -e MODEL_NAME=titanic_model \\\n",
    "    tensorflow/serving\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked, you should see output similar to\n",
    "\n",
    "```\n",
    "2020-11-29 17:26:24.774657: I tensorflow_serving/model_servers/server.cc:87] Building single TensorFlow model file config:  model_name: titanic_model model_base_path: /models/titanic_model\n",
    "2020-11-29 17:26:24.779699: I tensorflow_serving/model_servers/server_core.cc:464] Adding/updating models.\n",
    "2020-11-29 17:26:24.779750: I tensorflow_serving/model_servers/server_core.cc:575]  (Re-)adding model: titanic_model\n",
    "2020-11-29 17:26:24.894871: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: titanic_model version: 1}\n",
    "2020-11-29 17:26:24.894991: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: titanic_model version: 1}\n",
    "2020-11-29 17:26:24.895039: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: titanic_model version: 1}\n",
    "2020-11-29 17:26:24.896290: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/titanic_model/1\n",
    "2020-11-29 17:26:24.917508: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
    "2020-11-29 17:26:24.917573: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /models/titanic_model/1\n",
    "2020-11-29 17:26:24.920399: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
    "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "2020-11-29 17:26:24.977981: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\n",
    "2020-11-29 17:26:25.081348: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /models/titanic_model/1\n",
    "2020-11-29 17:26:25.104055: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 207773 microseconds.\n",
    "2020-11-29 17:26:25.107537: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /models/titanic_model/1/assets.extra/tf_serving_warmup_requests\n",
    "2020-11-29 17:26:25.114425: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: titanic_model version: 1}\n",
    "2020-11-29 17:26:25.123920: I tensorflow_serving/model_servers/server.cc:367] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
    "[warn] getaddrinfo: address family for nodename not supported\n",
    "2020-11-29 17:26:25.126379: I tensorflow_serving/model_servers/server.cc:387] Exporting HTTP/REST API at:localhost:8501 ...\n",
    "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now make a sample prediction as below. See https://www.tensorflow.org/tfx/serving/api_rest for details.\n",
    "```\n",
    "curl -d '{\"instances\": [{\"Pclass\":[3],\"Sex\":[\"male\"],\"Age\":[22.0],\"SibSp\":[1],\"Parch\":[0],\"Fare\":[7.25],\"Embarked\":[\"S\"]}]}' -X POST http://localhost:8501/v1/models/titanic_model:predict\n",
    "```\n",
    "If everything worked, you should see output similar to \n",
    "```\n",
    "{\n",
    "    \"predictions\": [[0.00236016512]]\n",
    "}\n",
    "```\n",
    "The predicted probability should match the result of the local prediction you ran in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use below helper to  generate the command for the selected sample passenger\n",
    "input_dict = {name: [value] for name, value in sample_passenger_dict.items()}\n",
    "instances = {\"instances\": [input_dict]}\n",
    "print(f\"curl -d '{json.dumps(instances)}' -X POST http://localhost:8501/v1/models/titanic_model:predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
